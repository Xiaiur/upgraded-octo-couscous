import sys
import warnings
warnings.filterwarnings('ignore')

import argparse
import json
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime

import torch
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TrainerCallback,
)
from torch.utils.tensorboard import SummaryWriter

from peft import LoraConfig, get_peft_model, TaskType, PeftConfig, PeftModel

# -------------------- –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è --------------------
class TrainingLogger:
    """–ö–ª–∞—Å—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, log_dir: str = "./logs", experiment_name: str = None):
        self.log_dir = log_dir
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ä–µ–¥—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        self.is_colab = 'google.colab' in sys.modules
        if self.is_colab:
            # –í Colab –ª—É—á—à–µ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ /content
            self.log_dir = '/content/logs'
            print(f"[Logger] –û–±–Ω–∞—Ä—É–∂–µ–Ω Google Colab, –ª–æ–≥–∏ –±—É–¥—É—Ç –≤ {self.log_dir}")
        
        if experiment_name is None:
            experiment_name = f"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.experiment_path = os.path.join(log_dir, experiment_name)
        os.makedirs(self.experiment_path, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TensorBoard
        self.tb_writer = SummaryWriter(log_dir=self.experiment_path)
        
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []
        self.steps = 0
        self.epochs = 0
        
        print(f"[Logger] –õ–æ–≥–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.experiment_path}")
    
    def log_step(self, 
                 loss: float, 
                 learning_rate: float = None,
                 grad_norm: float = None,
                 step: int = None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ"""
        if step is not None:
            self.steps = step
        else:
            self.steps += 1
            
        self.train_losses.append(loss)
        
        if learning_rate is not None:
            self.learning_rates.append(learning_rate)
            self.tb_writer.add_scalar('train/learning_rate', learning_rate, self.steps)
        
        if grad_norm is not None:
            self.tb_writer.add_scalar('train/grad_norm', grad_norm, self.steps)
        
        self.tb_writer.add_scalar('train/loss', loss, self.steps)
        self.tb_writer.add_scalar('train/step', self.steps, self.steps)
    
    def log_epoch(self, 
                  epoch: int, 
                  train_loss: float = None,
                  val_loss: float = None,
                  metrics: Dict = None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏"""
        self.epochs = epoch
        self.tb_writer.add_scalar('epoch', epoch, self.steps)
        
        if train_loss is not None:
            self.tb_writer.add_scalar('train/epoch_loss', train_loss, epoch)
        
        if val_loss is not None:
            self.val_losses.append(val_loss)
            self.tb_writer.add_scalar('val/loss', val_loss, epoch)
        
        if metrics:
            for key, value in metrics.items():
                self.tb_writer.add_scalar(f'val/{key}', value, epoch)
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        if epoch % 1 == 0:  # –ö–∞–∂–¥—É—é —ç–ø–æ—Ö—É
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∑–∞–º–µ–Ω–∏—Ç—å plot_progress() –Ω–∞ plot_progress_colab()
            self.plot_progress_colab()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –¥–ª—è Colab
    
    def plot_progress_colab(self):
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ Colab"""
        if not self.train_losses:
            return
        
        # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤ Colab
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # 1. –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
        axes[0].plot(self.train_losses, alpha=0.7, linewidth=1)
        axes[0].set_xlabel('–®–∞–≥–∏')
        axes[0].set_ylabel('–ü–æ—Ç–µ—Ä–∏ –æ–±—É—á–µ–Ω–∏—è')
        axes[0].set_title(f'–ü–æ—Ç–µ—Ä–∏ –æ–±—É—á–µ–Ω–∏—è (—à–∞–≥ {self.steps})')
        axes[0].grid(True, alpha=0.3)
        
        # 2. –°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏
        if len(self.train_losses) > 10:
            window = min(50, len(self.train_losses) // 4)
            moving_avg = pd.Series(self.train_losses).rolling(window=window).mean()
            axes[1].plot(moving_avg, label=f'–°—Ä–µ–¥–Ω–µ–µ (–æ–∫–Ω–æ={window})', 
                        color='red', linewidth=2)
            axes[1].plot(self.train_losses, alpha=0.2, label='–°—ã—Ä—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
            axes[1].set_xlabel('–®–∞–≥–∏')
            axes[1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
            axes[1].set_title('–°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
        
        plt.suptitle(f'–ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è (–≠–ø–æ—Ö–∞ {self.epochs}, –®–∞–≥–∏ {self.steps})', 
                    fontsize=12, fontweight='bold')
        plt.tight_layout()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ Colab
        plt.show()
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        plot_path = os.path.join(self.experiment_path, f'training_progress_colab_epoch_{self.epochs}.png')
        plt.savefig(plot_path, dpi=120, bbox_inches='tight')
        plt.close(fig)
    
    def plot_final_summary(self, model_name: str, config: Dict):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å—É–º–º–∞—Ä–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞"""
        if not self.train_losses:
            return
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # 1. –ü–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –ø–æ—Ç–µ—Ä—å
        axes[0].plot(self.train_losses, alpha=0.8, linewidth=1)
        if self.val_losses:
            val_steps = np.linspace(0, len(self.train_losses)-1, len(self.val_losses))
            axes[0].plot(val_steps, self.val_losses, 'o-', markersize=4, 
                        label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
        axes[0].set_xlabel('–®–∞–≥–∏')
        axes[0].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
        axes[0].set_title('–ü–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è')
        axes[0].grid(True, alpha=0.3)
        if self.val_losses:
            axes[0].legend()
        
        # 2. –°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ —Å –æ–∫–Ω–∞–º–∏
        axes[1].plot(self.train_losses, alpha=0.3, label='–°—ã—Ä—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
        windows = [10, 50, 100]
        colors = ['red', 'blue', 'green']
        for window, color in zip(windows, colors):
            if len(self.train_losses) > window:
                moving_avg = pd.Series(self.train_losses).rolling(window=window).mean()
                axes[1].plot(moving_avg, label=f'–û–∫–Ω–æ={window}', color=color, linewidth=2)
        axes[1].set_xlabel('–®–∞–≥–∏')
        axes[1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
        axes[1].set_title('–°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # 3. –ü–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—è)
        if self.val_losses:
            epochs = range(1, len(self.val_losses) + 1)
            axes[2].bar(epochs, self.val_losses, alpha=0.7)
            axes[2].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[2].set_ylabel('–ü–æ—Ç–µ—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
            axes[2].set_title('–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º')
            axes[2].grid(True, alpha=0.3, axis='y')
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for i, v in enumerate(self.val_losses):
                axes[2].text(i + 1, v, f'{v:.3f}', 
                           ha='center', va='bottom', fontsize=8)
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
            axes[2].hist(self.train_losses, bins=50, alpha=0.7)
            axes[2].set_xlabel('–ó–Ω–∞—á–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å')
            axes[2].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            axes[2].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Ç–µ—Ä—å')
            axes[2].grid(True, alpha=0.3)
        
        plt.suptitle(f'–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: {model_name}\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config}', 
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        final_plot_path = os.path.join(self.experiment_path, 'final_training_summary.png')
        plt.savefig(final_plot_path, dpi=150, bbox_inches='tight')
        plt.show()
        plt.close(fig)

    def display_colab_info(self):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π Colab"""
        if self.is_colab:
            print("\n" + "="*60)
            print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –î–õ–Ø GOOGLE COLAB:")
            print("="*60)
            print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è: {self.experiment_path}/")
            print(f"üìÅ –õ–æ–≥–∏ TensorBoard: {self.experiment_path}")
            print("="*60)
            print("\n–ß—Ç–æ–±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å TensorBoard –≤ Colab, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
            print(f"  %load_ext tensorboard")
            print(f"  %tensorboard --logdir {self.experiment_path}")
            print("="*60 + "\n")    
    
    def save_logs(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –≤ —Ñ–∞–π–ª"""
        log_data = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'total_steps': self.steps,
            'total_epochs': self.epochs,
            'timestamp': datetime.now().isoformat()
        }
        
        log_file = os.path.join(self.experiment_path, 'training_logs.json')
        with open(log_file, 'w') as f:
            json.dump(log_data, f, indent=2)
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ CSV –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        df = pd.DataFrame({
            'step': range(len(self.train_losses)),
            'train_loss': self.train_losses,
            'learning_rate': (self.learning_rates + 
                            [None] * (len(self.train_losses) - len(self.learning_rates))) 
                            if self.learning_rates else [None] * len(self.train_losses)
        })
        csv_file = os.path.join(self.experiment_path, 'training_logs.csv')
        df.to_csv(csv_file, index=False)
        
        print(f"[Logger] –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_file}")
        print(f"[Logger] CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {csv_file}")
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–≥–≥–µ—Ä–∞"""
        self.tb_writer.close()
        self.save_logs()


class CustomTrainerCallback(TrainerCallback):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Trainer"""
    
    def __init__(self, logger: TrainingLogger):
        self.logger = logger
        self.is_colab = 'google.colab' in sys.modules
        self.last_log_time = datetime.now()
        self.progress_interval = 50  # –®–∞–≥–æ–≤ –º–µ–∂–¥—É –≤—ã–≤–æ–¥–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ Colab
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏"""
        if logs is not None:
            loss = logs.get('loss')
            learning_rate = logs.get('learning_rate')
            grad_norm = logs.get('grad_norm')
            
            if loss is not None:
                self.logger.log_step(
                    loss=loss,
                    learning_rate=learning_rate,
                    grad_norm=grad_norm,
                    step=state.global_step
                )
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ Colab
                if self.is_colab and state.global_step % self.progress_interval == 0:
                    current_time = datetime.now()
                    elapsed = (current_time - self.last_log_time).total_seconds()
                    
                    print(f"\n[–®–∞–≥ {state.global_step}] –ü–æ—Ç–µ—Ä—è: {loss:.4f} | "
                          f"LR: {learning_rate:.2e} | "
                          f"–í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ: {elapsed:.1f}—Å")
                    
                    self.last_log_time = current_time
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏"""
        epoch = state.epoch
        train_loss = state.log_history[-1].get('loss') if state.log_history else None
        
        self.logger.log_epoch(
            epoch=int(epoch),
            train_loss=train_loss
        )
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è Colab
        if self.is_colab:
            print(f"\n{'='*50}")
            print(f"–≠–ü–û–•–ê {int(epoch)} –ó–ê–í–ï–†–®–ï–ù–ê")
            print(f"{'='*50}")
            if train_loss:
                print(f"–°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è –∑–∞ —ç–ø–æ—Ö—É: {train_loss:.4f}")
            print(f"–í—Å–µ–≥–æ —à–∞–≥–æ–≤: {state.global_step}")
            print(f"–¢–µ–∫—É—â–∏–π learning rate: {args.learning_rate:.2e}")
            print(f"{'='*50}\n")
def save_to_drive_in_colab(output_dir, experiment_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Google Drive (—Ç–æ–ª—å–∫–æ –≤ Colab)"""
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        
        drive_path = '/content/drive/MyDrive/LLM_Training'
        os.makedirs(drive_path, exist_ok=True)
        
        # –ö–æ–ø–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        import shutil
        model_drive_path = os.path.join(drive_path, os.path.basename(output_dir))
        shutil.copytree(output_dir, model_drive_path, dirs_exist_ok=True)
        
        # –ö–æ–ø–∏—Ä—É–µ–º –ª–æ–≥–∏
        logs_drive_path = os.path.join(drive_path, 'logs', os.path.basename(experiment_path))
        shutil.copytree(experiment_path, logs_drive_path, dirs_exist_ok=True)
        
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Google Drive:")
        print(f"   –ú–æ–¥–µ–ª—å: {model_drive_path}")
        print(f"   –õ–æ–≥–∏: {logs_drive_path}")
        
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Google Drive: {e}")
        return False

# -------------------- –£—Ç–∏–ª–∏—Ç—ã --------------------
def read_markdown(path: str) -> str:
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def prepare_markdown_dataset(
    text: str,
    tokenizer: AutoTokenizer,
    max_length: int = 512,
    stride: int = 128,
    test_size: float = 0.1
):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ markdown —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ train/validation"""
    tokenized = tokenizer(
        text,
        return_tensors=None,
        add_special_tokens=False
    )["input_ids"]

    records = []

    for start in range(0, len(tokenized) - max_length, max_length - stride):
        chunk = tokenized[start:start + max_length]

        records.append({
            "input_ids": chunk,
            "attention_mask": [1] * len(chunk),
            "labels": chunk.copy(),
        })

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ validation
    total_samples = len(records)
    split_idx = int(total_samples * (1 - test_size))
    
    train_records = records[:split_idx]
    val_records = records[split_idx:] if test_size > 0 else []
    
    train_ds = Dataset.from_list(train_records)
    val_ds = Dataset.from_list(val_records) if val_records else None
    
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(train_records)} train –∏ {len(val_records)} validation –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    return train_ds, val_ds

def read_json_or_jsonl(path: str) -> List[Dict[str, str]]:
    with open(path, 'r', encoding='utf-8') as f:
        text = f.read().strip()
        if not text:
            return []
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ JSON —Ü–µ–ª–∏–∫–æ–º (—Å–ø–∏—Å–æ–∫)
        try:
            obj = json.loads(text)
            if isinstance(obj, list):
                return obj
        except Exception:
            pass
        # –ò–Ω–∞—á–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ JSONL
        items = []
        with open(path, 'r', encoding='utf-8') as fr:
            for line in fr:
                line = line.strip()
                if not line:
                    continue
                items.append(json.loads(line))
        return items


def get_default_lora_targets(model_name: str) -> List[str]:
    mn = model_name.lower()
    if 'llama' in mn or 'alpaca' in mn:
        return ["q_proj", "k_proj", "v_proj", "o_proj"]
    if 'gpt2' in mn or 'gpt' in mn or 'dialo' in mn:
        return ["c_attn", "c_proj", "c_fc", "c_ffn"]
    # Qwen/Falcon/–∏–Ω—ã–µ: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
    return ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']


# -------------------- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö --------------------
def prepare_dataset(items: List[Dict[str, str]], tokenizer: AutoTokenizer, max_length: int = 512, test_size: float = 0.1):
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º prompt –∏ response –æ—Ç–¥–µ–ª—å–Ω–æ, —Å–æ–±–∏—Ä–∞–µ–º input_ids –∏ labels
    labels: -100 –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ prompt (–∏ sep), —Ä–µ–∞–ª—å–Ω—ã–µ id –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ response
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç train –∏ validation –¥–∞—Ç–∞—Å–µ—Ç—ã
    """
    records = []

    # –≤—ã–±–µ—Ä–µ–º SEP –∫–∞–∫ eos_token (–µ—Å–ª–∏ –µ—Å—Ç—å), –∏–Ω–∞—á–µ "\n"
    if tokenizer.eos_token_id is not None:
        sep_token_id = tokenizer.eos_token_id
        sep_text = tokenizer.eos_token
    else:
        # –∫–∞–∫ fallback ‚Äî –¥–æ–±–∞–≤–∏–º –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ (–±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω)
        sep_token_id = None
        sep_text = "\n"

    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        "total_examples": 0,
        "empty_responses": 0,
        "truncated_prompts": 0,
        "all_ignored_labels": 0,
        "avg_trainable_tokens": 0,
    }
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–µ—Ä–≤—ã—Ö N –ø—Ä–∏–º–µ—Ä–æ–≤
    debug_first_n = min(5, len(items))
    print(f"\nüîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ø–µ—Ä–≤—ã—Ö {debug_first_n} –ø—Ä–∏–º–µ—Ä–æ–≤:")
    print("-" * 50)

    for idx, it in enumerate(items):
        prompt = it.get('context', '') or it.get('instruction', '') or ''
        response = it.get('utterance', '') or it.get('output', '') or it.get('response', '') or ''
        
        stats["total_examples"] += 1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π response
        if not response.strip():
            stats["empty_responses"] += 1
            if idx < debug_first_n:
                print(f"–ü—Ä–∏–º–µ—Ä #{idx}: –ü–£–°–¢–û–ô response, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
            continue
            
        # —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –±–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        enc_prompt = tokenizer(prompt, add_special_tokens=False)
        enc_resp = tokenizer(response, add_special_tokens=False)

        prompt_ids = enc_prompt["input_ids"]
        resp_ids = enc_resp["input_ids"]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: response —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª—Å—è –≤ –ø—É—Å—Ç–æ—Ç—É?
        if len(resp_ids) == 0:
            if idx < debug_first_n:
                print(f"–ü—Ä–∏–º–µ—Ä #{idx}: Response —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª—Å—è –≤ –ø—É—Å—Ç–æ—Ç—É!")
                print(f"  Response —Ç–µ–∫—Å—Ç: '{response[:50]}...'")
                print(f"  –¢–æ–∫–µ–Ω—ã: {tokenizer.tokenize(response[:50])}")
            continue

        # —Å–æ—Å—Ç–∞–≤–ª—è–µ–º input_ids: prompt + [sep?] + response
        if sep_token_id is not None:
            input_ids = prompt_ids + [sep_token_id] + resp_ids
            prompt_len = len(prompt_ids) + 1
        else:
            # –µ—Å–ª–∏ –Ω–µ—Ç eos_token_id ‚Äî –ø–æ–ª–æ–∂–∏–º —è–≤–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∫–∞–∫ –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            sep_enc = tokenizer(sep_text, add_special_tokens=False)
            sep_ids = sep_enc["input_ids"]
            input_ids = prompt_ids + sep_ids + resp_ids
            prompt_len = len(prompt_ids) + len(sep_ids)

        original_length = len(input_ids)
        
        # –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —É—Å–µ–∫–∞–µ–º –¥–æ max_length –° –ö–û–ù–¶–ê (—á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å prompt)
        if len(input_ids) > max_length:
            # –û–±—Ä–µ–∑–∞–µ–º —Å –∫–æ–Ω—Ü–∞, —Å–æ—Ö—Ä–∞–Ω—è—è prompt
            if prompt_len <= max_length:
                # prompt –ø–æ–º–µ—â–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é, –æ–±—Ä–µ–∑–∞–µ–º —Ç–æ–ª—å–∫–æ response
                input_ids = input_ids[:max_length]
            else:
                # prompt —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –æ–±—Ä–µ–∑–∞—Ç—å –∏ –µ–≥–æ
                input_ids = input_ids[:max_length]
                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º prompt_len –¥–ª—è –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
                # –ù–∞—Ö–æ–¥–∏–º, –≥–¥–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è prompt –≤ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                # –≠—Ç–æ –º–∏–Ω–∏–º—É–º –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º prompt_len –∏ max_length
                prompt_len = min(prompt_len, max_length)
                stats["truncated_prompts"] += 1
        
        # –†–ê–°–ß–ï–¢ labels: -100 –¥–ª—è prompt, —Ä–µ–∞–ª—å–Ω—ã–µ id –¥–ª—è response
        if prompt_len >= len(input_ids):
            # –°–ª—É—á–∞–π 1: prompt –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–ª–∏ –±–æ–ª—å—à–µ
            labels = [-100] * len(input_ids)
            if idx < debug_first_n:
                print(f"–ü—Ä–∏–º–µ—Ä #{idx}: prompt –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å!")
        else:
            # –°–ª—É—á–∞–π 2: –µ—Å—Ç—å response –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            labels = [-100] * prompt_len + input_ids[prompt_len:]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: –≤—Å–µ –ª–∏ labels = -100?
        trainable_tokens = sum(1 for label in labels if label != -100)
        if trainable_tokens == 0:
            stats["all_ignored_labels"] += 1
        
        stats["avg_trainable_tokens"] += trainable_tokens
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        if idx < debug_first_n:
            num_ignored = len(labels) - trainable_tokens
            print(f"\n–ü—Ä–∏–º–µ—Ä #{idx}:")
            print(f"  Prompt: '{prompt[:50]}{'...' if len(prompt) > 50 else ''}'")
            print(f"  Response: '{response[:50]}{'...' if len(response) > 50 else ''}'")
            print(f"  –î–ª–∏–Ω–∞ prompt: {len(prompt_ids)} —Ç–æ–∫–µ–Ω–æ–≤")
            print(f"  –î–ª–∏–Ω–∞ response: {len(resp_ids)} —Ç–æ–∫–µ–Ω–æ–≤")
            print(f"  –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {original_length}, –ø–æ—Å–ª–µ —É—Å–µ—á–µ–Ω–∏—è: {len(input_ids)}")
            print(f"  prompt_len: {prompt_len}")
            print(f"  –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {num_ignored}")
            print(f"  –û–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {trainable_tokens}")
            
            if trainable_tokens == 0:
                print(f"  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
                if prompt_len >= len(input_ids):
                    print(f"    –ü—Ä–∏—á–∏–Ω–∞: prompt_len ({prompt_len}) >= –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ({len(input_ids)})")
                elif len(resp_ids) == 0:
                    print(f"    –ü—Ä–∏—á–∏–Ω–∞: response —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª—Å—è –≤ –ø—É—Å—Ç–æ—Ç—É")
            else:
                # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 10 –æ–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                trainable_positions = [i for i, label in enumerate(labels) if label != -100]
                first_trainable = trainable_positions[:5]
                print(f"  –ü–µ—Ä–≤—ã–µ –æ–±—É—á–∞–µ–º—ã–µ –ø–æ–∑–∏—Ü–∏–∏: {first_trainable}")
        
        # attention_mask ‚Äî –≤—Å–µ 1 (–±—É–¥–µ—Ç –ø–∞–¥–¥–∏—Ç—å—Å—è –≤ collator)
        attention_mask = [1] * len(input_ids)

        records.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
        })

    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–ù–ù–´–•:")
    print(f"  –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {stats['total_examples']}")
    print(f"  –ü—É—Å—Ç—ã—Ö responses: {stats['empty_responses']}")
    print(f"  –£—Å–µ—á–µ–Ω–Ω—ã—Ö prompts: {stats['truncated_prompts']}")
    print(f"  –ü—Ä–∏–º–µ—Ä–æ–≤ –±–µ–∑ –æ–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {stats['all_ignored_labels']}")
    
    if stats['total_examples'] - stats['all_ignored_labels'] > 0:
        avg_trainable = stats['avg_trainable_tokens'] / (stats['total_examples'] - stats['all_ignored_labels'])
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –æ–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –ø—Ä–∏–º–µ—Ä: {avg_trainable:.1f}")
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    if stats['all_ignored_labels'] == stats['total_examples']:
        print(f"\nüö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –í–æ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –Ω–µ—Ç –æ–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤!")
        print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print(f"   1. –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö (–∫–ª—é—á–∏ 'context', 'utterance', 'response', 'output')")
        print(f"   2. –ß—Ç–æ response –Ω–µ –ø—É—Å—Ç")
        print(f"   3. –ß—Ç–æ prompt –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (max_length={max_length})")
        return None, None
    
    if stats['all_ignored_labels'] > stats['total_examples'] * 0.5:
        print(f"\n‚ö†Ô∏è  –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ë–æ–ª–µ–µ 50% –ø—Ä–∏–º–µ—Ä–æ–≤ –±–µ–∑ –æ–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤!")
        print(f"   –û–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º.")

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ validation
    if len(records) == 0:
        print(f"\n‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None, None
        
    total_samples = len(records)
    split_idx = int(total_samples * (1 - test_size))
    
    train_records = records[:split_idx]
    val_records = records[split_idx:] if test_size > 0 else []
    
    train_ds = Dataset.from_list(train_records)
    val_ds = Dataset.from_list(val_records) if val_records else None
    
    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(train_records)} train –∏ {len(val_records)} validation –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    if train_ds and len(train_ds) > 0:
        print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ train –ø—Ä–∏–º–µ—Ä–∞:")
        sample = train_ds[0]
        print(f"  –î–ª–∏–Ω–∞ input_ids: {len(sample['input_ids'])}")
        print(f"  –î–ª–∏–Ω–∞ labels: {len(sample['labels'])}")
        trainable = sum(1 for label in sample['labels'] if label != -100)
        print(f"  –û–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {trainable}")
        
        if trainable > 0:
            # –ü–æ–∫–∞–∂–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã
            positions = [i for i, label in enumerate(sample['labels']) if label != -100][:10]
            print(f"  –ü–æ–∑–∏—Ü–∏–∏ –æ–±—É—á–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {positions}")
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç response
            response_start = positions[0] if positions else len(sample['labels']) - 5
            response_tokens = sample['input_ids'][response_start:response_start+10]
            try:
                decoded = tokenizer.decode(response_tokens, skip_special_tokens=True)
                print(f"  –ü—Ä–∏–º–µ—Ä response: '{decoded[:100]}...'")
            except:
                pass
    
    return train_ds, val_ds


@dataclass
class DataCollatorForCausalLMWithLabels:
    tokenizer: AutoTokenizer
    max_length: int = 512

    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:
        # 1. –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–∫–∏
        input_ids = [torch.tensor(f["input_ids"], dtype=torch.long) for f in features]
        attention_mask = [torch.tensor(f["attention_mask"], dtype=torch.long) for f in features]
        labels = [torch.tensor(f["labels"], dtype=torch.long) for f in features]

        # 2. –ù–∞—Ö–æ–¥–∏–º max –¥–ª–∏–Ω—É –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞
        max_len = max(x.size(0) for x in input_ids)

        # 3. –ü–∞–¥–¥–∏–º –≤—Ä—É—á–Ω—É—é
        def pad(tensor, pad_value):
            return torch.nn.functional.pad(
                tensor,
                (0, max_len - tensor.size(0)),
                value=pad_value
            )

        pad_id = self.tokenizer.pad_token_id
        if pad_id is None:
            # fallback
            pad_id = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id is not None else 0

        input_ids = torch.stack([pad(x, pad_id) for x in input_ids])
        attention_mask = torch.stack([pad(x, 0) for x in attention_mask])
        labels = torch.stack([pad(x, -100) for x in labels])

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }


def main():
    parser = argparse.ArgumentParser(description='Fine-tune causal LM with LoRA (PEFT)')

    parser.add_argument('--model_name', type=str, default='gpt2', help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ HF')
    parser.add_argument('--data_path', type=str, required=True, help='–ü—É—Ç—å –∫ data.json –∏–ª–∏ data.jsonl –∏–ª–∏ .md —Ñ–∞–π–ª—É')
    parser.add_argument('--output_dir', type=str, default='./fine-tuned-lora', help='–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å/–∞–¥–∞–ø—Ç–µ—Ä')
    parser.add_argument('--log_dir', type=str, default='./logs', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤')
    parser.add_argument('--experiment_name', type=str, default=None, help='–ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞')
    parser.add_argument('--per_device_train_batch_size', type=int, default=4)
    parser.add_argument('--num_train_epochs', type=int, default=3)
    parser.add_argument('--learning_rate', type=float, default=2e-4)
    parser.add_argument('--max_length', type=int, default=512)
    parser.add_argument('--use_lora', action='store_true')
    parser.add_argument('--lora_r', type=int, default=16)
    parser.add_argument('--lora_alpha', type=int, default=64)
    parser.add_argument('--lora_dropout', type=float, default=0.2)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--logging_steps', type=int, default=10, help='–ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--eval_steps', type=int, default=100, help='–ß–∞—Å—Ç–æ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (0 - –æ—Ç–∫–ª—é—á–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é)')
    parser.add_argument('--save_steps', type=int, default=500, help='–ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    parser.add_argument('--test_size', type=float, default=0.1, help='–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (0.0-0.3)')
    parser.add_argument('--no_eval', action='store_true', help='–ü–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é')
    parser.add_argument('--save_to_drive', action='store_true', 
                       help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Google Drive (—Ç–æ–ª—å–∫–æ –¥–ª—è Colab)')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, 
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞')
    parser.add_argument('--warmup_steps', type=int, default=100, 
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞')
    parser.add_argument('--max_grad_norm', type=float, default=1.0, 
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –æ–±—Ä–µ–∑–∫–∏')
    parser.add_argument('--optimizer', type=str, default='adamw_torch', 
                       choices=['adamw_torch', 'adamw_apex_fused', 'adafactor'],
                       help='–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--lr_scheduler_type', type=str, default='cosine',
                       choices=['linear', 'cosine', 'cosine_with_restarts', 'constant', 'constant_with_warmup'],
                       help='–¢–∏–ø –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ learning rate')

    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤ Colab –ª–∏ –º—ã
    is_colab = 'google.colab' in sys.modules
    if is_colab:
        print("="*60)
        print("üöÄ –ó–ê–ü–£–°–ö –í GOOGLE COLAB")
        print("="*60)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—É—Ç–∏ –¥–ª—è Colab
        if args.log_dir == "./logs":
            args.log_dir = "/content/logs"
        if args.output_dir == "./fine-tuned-lora":
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            args.output_dir = f"/content/fine-tuned-lora-{timestamp}"
        
        print(f"üìÅ –õ–æ–≥–∏: {args.log_dir}")
        print(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {args.output_dir}")
        print("="*60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
    logger = TrainingLogger(
        log_dir=args.log_dir,
        experiment_name=args.experiment_name
    )
    logger.display_colab_info()
    
    # –°–±–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞
    config = {
        'model_name': args.model_name,
        'data_path': args.data_path,
        'use_lora': args.use_lora,
        'lora_r': args.lora_r,
        'lora_alpha': args.lora_alpha,
        'lora_dropout': args.lora_dropout,
        'learning_rate': args.learning_rate,
        'batch_size': args.per_device_train_batch_size,
        'num_epochs': args.num_train_epochs,
        'max_length': args.max_length,
        'seed': args.seed,
        'test_size': args.test_size,
        'eval_steps': args.eval_steps,
        'gradient_accumulation_steps': args.gradient_accumulation_steps,
        'warmup_steps': args.warmup_steps,
        'optimizer': args.optimizer,
        'lr_scheduler_type': args.lr_scheduler_type,
        'max_grad_norm': args.max_grad_norm
    }
    
    print(f"\nüìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
    print("-" * 40)
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("-" * 40)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–¥—ã –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏: {args.model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)
        
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ pad_token
        if tokenizer.pad_token is None:
            if tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
                print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º eos_token ({tokenizer.eos_token}) –∫–∞–∫ pad_token")
            else:
                tokenizer.add_special_tokens({"pad_token": "[PAD]"})
                print("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω pad_token: [PAD]")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        print(f"üìù –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω:")
        print(f"   pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
        print(f"   eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})")
        print(f"   bos_token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        print(f"ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {args.model_name}...")
        model = AutoModelForCausalLM.from_pretrained(
            args.model_name,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True  # –î–ª—è –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
        )
        
        # –î–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –≤—ã–∫–ª—é—á–∞–µ–º –∫—ç—à –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        try:
            model.gradient_checkpointing_enable()
            print("‚úÖ –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å gradient checkpointing: {e}")
        
        model.config.use_cache = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
        if args.use_lora:
            target_modules = get_default_lora_targets(args.model_name)
            print(f"\nüéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º target_modules –¥–ª—è LoRA: {target_modules}")
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=args.lora_r,
                lora_alpha=args.lora_alpha,
                lora_dropout=args.lora_dropout,
                target_modules=target_modules,
                bias="none",
            )
            model = get_peft_model(model, lora_config)
            # –ø–æ–∫–∞–∂–µ–º —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–∞–µ—Ç—Å—è
            model.print_trainable_parameters()
        else:
            print("‚ö†Ô∏è LoRA –æ—Ç–∫–ª—é—á–µ–Ω, –æ–±—É—á–∞—é—Ç—Å—è –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏/—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        logger.close()
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
    print(f"\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {args.data_path}")
    
    if not os.path.exists(args.data_path):
        print(f"‚ùå –§–∞–π–ª {args.data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        logger.close()
        return
    
    try:
        if args.data_path.endswith('.md'):
            train_ds, eval_ds = prepare_markdown_dataset(
                text=read_markdown(args.data_path),
                tokenizer=tokenizer,
                max_length=args.max_length,
                test_size=0 if args.no_eval else args.test_size
            )
        else:
            items = read_json_or_jsonl(args.data_path)
            print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(items)} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ñ–∞–π–ª–∞")
            if items:
                # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
                print("\nüìã –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–π –∑–∞–ø–∏—Å–∏:")
                for key in items[0].keys():
                    print(f"   {key}: {items[0][key][:100]}...")
            
            train_ds, eval_ds = prepare_dataset(
                items, 
                tokenizer, 
                args.max_length,
                test_size=0 if args.no_eval else args.test_size
            )
        
        print(f"\nüìä –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        if eval_ds:
            print(f"üìä –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(eval_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            print("\nüîç –ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            sample = train_ds[0]
            print(f"   input_ids –¥–ª–∏–Ω–∞: {len(sample['input_ids'])}")
            print(f"   labels: {sample['labels'][:20]}...")
            print(f"   –ù–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {sum(1 for x in sample['labels'] if x != -100)}")
        else:
            print("‚ö†Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ —Å–æ–∑–¥–∞–Ω")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        logger.close()
        return
    
    data_collator = DataCollatorForCausalLMWithLabels(tokenizer=tokenizer)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    if args.no_eval or args.eval_steps <= 0 or eval_ds is None:
        eval_strategy = "no"
        eval_steps = None
        load_best_model_at_end = False
        print("\n‚ö†Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    else:
        eval_strategy = "steps"
        eval_steps = args.eval_steps
        load_best_model_at_end = True
        print(f"\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞ –∫–∞–∂–¥—ã–µ {eval_steps} —à–∞–≥–æ–≤")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    print(f"\n‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_train_batch_size * 2,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        logging_steps=args.logging_steps,
        eval_steps=eval_steps,
        save_steps=args.save_steps,
        save_strategy="steps",
        eval_strategy=eval_strategy,
        save_total_limit=3,
        fp16=True,
        remove_unused_columns=False,
        push_to_hub=False,
        report_to=[],
        load_best_model_at_end=load_best_model_at_end,
        metric_for_best_model="loss" if eval_ds else None,
        greater_is_better=False,
        logging_dir=logger.experiment_path,
        seed=args.seed,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_steps=args.warmup_steps,
        optim=args.optimizer,
        lr_scheduler_type=args.lr_scheduler_type,
        max_grad_norm=args.max_grad_norm,
        group_by_length=False,
        ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,
        dataloader_num_workers=0,
        dataloader_pin_memory=True,
        eval_accumulation_steps=None,
        prediction_loss_only=True,
    )

    # –°–æ–∑–¥–∞–µ–º Trainer
    trainer_kwargs = {
        "model": model,
        "args": training_args,
        "train_dataset": train_ds,
        "data_collator": data_collator,
        "callbacks": [CustomTrainerCallback(logger)],
    }
    
    if eval_ds and eval_strategy != "no":
        trainer_kwargs["eval_dataset"] = eval_ds
    
    trainer = Trainer(**trainer_kwargs)

    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    print(f"   –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {len(train_ds) * args.num_train_epochs // (args.per_device_train_batch_size * args.gradient_accumulation_steps)}")
    print(f"   –ì—Ä–∞—Ñ–∏–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏: {logger.experiment_path}/")
    print("-" * 60)
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_file = os.path.join(logger.experiment_path, 'training_config.json')
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"üìù –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {config_file}")

        print("\nüîç –ü–†–û–í–ï–†–ö–ê DATA COLLATOR:")
        # –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        sample_batch = [train_ds[i] for i in range(2)]
        collated = data_collator(sample_batch)

        print(f"input_ids shape: {collated['input_ids'].shape}")
        print(f"labels shape: {collated['labels'].shape}")

        # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ labels –Ω–µ –≤—Å–µ -100
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ labels: {torch.unique(collated['labels'])[:10].tolist()}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ -100 –≤ –ø–µ—Ä–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ: {(collated['labels'][0] == -100).sum().item()}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–µ—Ä–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ: {(collated['labels'][0] != -100).sum().item()}")

        # –ü—Ä–æ–≤–µ—Ä–∏–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        print(f"\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
        print(f"  input_ids dtype: {collated['input_ids'].dtype}")
        print(f"  labels dtype: {collated['labels'].dtype}")
        print(f"  attention_mask dtype: {collated['attention_mask'].dtype}")
        
        train_result = trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        metrics = train_result.metrics
        metrics_file = os.path.join(logger.experiment_path, 'training_metrics.json')
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {metrics_file}")
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {metrics}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –æ—Ç—á–µ—Ç
        logger.plot_final_summary(args.model_name, config)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ—Ä–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        interrupted_dir = args.output_dir + "_interrupted"
        os.makedirs(interrupted_dir, exist_ok=True)
        
        if args.use_lora:
            model.save_pretrained(interrupted_dir)
            tokenizer.save_pretrained(interrupted_dir)
            print(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {interrupted_dir}")
        
        logger.close()
        
        # –ö–æ–ø–∏—Ä—É–µ–º –ª–æ–≥–∏
        import shutil
        logs_in_output = os.path.join(interrupted_dir, 'training_logs')
        shutil.copytree(logger.experiment_path, logs_in_output, dirs_exist_ok=True)
        
        if args.save_to_drive and is_colab:
            save_to_drive_in_colab(interrupted_dir, logger.experiment_path)
        
        print("–û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")
        return
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ
        error_dir = args.output_dir + "_error"
        os.makedirs(error_dir, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
        error_info = {
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'config': config
        }
        error_file = os.path.join(logger.experiment_path, 'error_info.json')
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(error_info, f, indent=2, ensure_ascii=False)
        
        logger.close()
        raise
    
    finally:
        # –í—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä
        logger.close()

    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ {args.output_dir}...")
    os.makedirs(args.output_dir, exist_ok=True)

    # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ LoRA: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¢–û–õ–¨–ö–û –∞–¥–∞–ø—Ç–µ—Ä
    if args.use_lora:
        model.save_pretrained(args.output_dir)
        tokenizer.save_pretrained(args.output_dir)
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω LoRA-–∞–¥–∞–ø—Ç–µ—Ä –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ {args.output_dir}")
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LoRA
        lora_config_file = os.path.join(args.output_dir, 'adapter_config.json')
        if os.path.exists(lora_config_file):
            with open(lora_config_file, 'r', encoding='utf-8') as f:
                lora_cfg = json.load(f)
            print(f"üìÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA: r={lora_cfg.get('r')}, alpha={lora_cfg.get('lora_alpha')}")
    else:
        model.save_pretrained(args.output_dir, safe_serialization=True)
        tokenizer.save_pretrained(args.output_dir)
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ {args.output_dir}")

    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\nüìà –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç...")
    logger.plot_final_summary(args.model_name, config)
    
    # –ö–æ–ø–∏—Ä—É–µ–º –ª–æ–≥–∏ –≤ output_dir
    import shutil
    logs_in_output = os.path.join(args.output_dir, 'training_logs')
    shutil.copytree(logger.experiment_path, logs_in_output, dirs_exist_ok=True)
    print(f"üìä –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {logs_in_output}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Google Drive –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if args.save_to_drive and is_colab:
        print("\n‚òÅÔ∏è –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Google Drive...")
        if save_to_drive_in_colab(args.output_dir, logger.experiment_path):
            print("‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Google Drive!")
        else:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Google Drive")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    print("\n" + "="*60)
    print("üéâ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*60)
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
    print(f"   –ú–æ–¥–µ–ª—å: {args.output_dir}")
    print(f"   –õ–æ–≥–∏: {logs_in_output}")
    print(f"   –ì—Ä–∞—Ñ–∏–∫–∏: {logger.experiment_path}/")
    
    if eval_ds:
        print(f"\nüìà –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
        print(f"   –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è: {min(logger.val_losses) if logger.val_losses else 'N/A':.4f}")
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è:")
    print(f"   –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {logger.steps}")
    print(f"   –í—Å–µ–≥–æ —ç–ø–æ—Ö: {logger.epochs}")
    print(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ–±—É—á–∞–µ–º—ã—Ö): {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –≥—Ä–∞—Ñ–∏–∫—É
    if is_colab:
        latest_plot = os.path.join(logger.experiment_path, 'final_training_summary.png')
        if os.path.exists(latest_plot):
            print(f"\nüì∏ –§–∏–Ω–∞–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫: {latest_plot}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ –ø—Ä—è–º–æ –≤ –≤—ã–≤–æ–¥–µ (—Ç–æ–ª—å–∫–æ –≤ Colab)
            try:
                from IPython.display import Image, display
                print("\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
                display(Image(filename=latest_plot))
            except:
                print("(–ì—Ä–∞—Ñ–∏–∫ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏)")
    
    print("="*60)
    print("–ì–æ—Ç–æ–≤–æ! ‚úÖ")


if __name__ == '__main__':
    main()
